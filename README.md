# Jeevita Kulkarni ‚Äì Data Engineer Portfolio

## üëã About Me
I‚Äôm a Data Engineer with 1.5 years of experience designing and building end-to-end data solutions. I work independently to transform raw data into actionable insights, creating automated pipelines, optimized queries, and interactive dashboards.
I have experience across multiple tools and technologies, including Python, SQL, Airflow, BigQuery, Databricks DLT, and visualization tools like Power BI. For each project, I select the technologies that best fit the problem, ensuring practical, scalable, and maintainable solutions.

My focus is on solving real-world data challenges with clean, efficient pipelines and insightful analytics, demonstrating both versatility and depth in my work. 

**Tech Stack:** Python | SQL | Airflow | BigQuery | Databricks | Delta Live Tables (DLT) | Snowflake | Power BI  

---

## üöÄ Key Projects

### **1. Swiggy Data Pipeline**  
**Role:** Independent Developer  
**Tech:** Python, Airflow, BigQuery  
**Project Overview:**  
Built an automated ETL pipeline to process and clean Swiggy delivery data, enabling efficient reporting.  

**Impact / Highlights:**  
- Automated daily data ingestion & cleaning workflows  
- Reduced pipeline runtime by **35%**  
- Enabled **real-time reporting** for delivery metrics  

[View Project Folder](https://github.com/Jeevitak/Data_Engineer_Portfolio/tree/main/Swiggy_Pipeline)  

---

### **2. Sales Analytics Pipeline**  
**Role:** Independent Developer  
**Tech:** Python, BigQuery, Looker Studio  
**Project Overview:**  
Developed a pipeline and dashboards to track monthly sales and KPIs for internal analytics purposes.  

**Impact / Highlights:**  
- Built dashboards for internal reporting and KPI tracking  
- Automated data pipelines to reduce manual effort  
- Provided actionable insights for business metrics  


[View Project Folder](https://github.com/Jeevitak/Data_Engineer_Portfolio/tree/main/Airflow_Sales_Analytics_Pipeline)  

---

### **3. Databricks DLT Pipeline Project**  
**Role:** Independent Developer  
**Tech:** Databricks, Delta Live Tables (DLT), Python, SQL, BigQuery  
**Project Overview:**  
Built an end-to-end pipeline using **DLT with Bronze, Silver, and Gold layers**. Implemented **batch and streaming ingestion** for raw data to create analytics-ready datasets.  

**Impact / Highlights:**  
- Designed a scalable and modular pipeline for data transformation and enrichment  
- Automated ingestion and processing across all layers  
- Prepared clean datasets for downstream reporting and analytics  
 
[View Project Folder](https://github.com/Jeevitak/Data_Engineer_Portfolio/tree/main/Databricks_Project)  

---

## üìä Skills & Expertise
- **ETL & Data Pipelines:** Airflow, Python, BigQuery, Databricks, DLT  
- **Analytics & Dashboards:** Looker Studio, Tableau, Power BI  
- **SQL Optimization:** Complex queries, performance tuning  
- **Version Control & Collaboration:** Git, GitHub  

---

**Project: COVID-19 Data Pipeline & Dashboard on Snowflake**  

Role: Independent Developer  
Tech: Snowflake, Python, Streamlit, Plotly  
Project Overview:  
Built an automated COVID-19 data pipeline on Snowflake with incremental loads, clustering for faster queries, and an interactive Streamlit dashboard for analysis and reporting.  

Impact / Highlights:  
- Implemented incremental data loads using Snowflake streams and tasks  
- Optimized queries using table clustering for faster reporting  
- Developed an interactive dashboard with Streamlit and Plotly  

[View Project Folder](Data_Analysing_organising_with_snowtasks_and_streamlit)


# Real-Time Crypto Data Pipeline üöÄ

### üìå Overview
A real-time data pipeline built to stream **cryptocurrency price data** from the CoinGecko API into **Snowflake** using **Kafka** and **Airflow**.

### ‚öôÔ∏è Tech Stack
- Python (Kafka Producer & Consumer)
- Apache Kafka & Zookeeper (Dockerized)
- Apache Airflow (workflow orchestration)
- Snowflake (data warehouse)
- Snowsight (reporting & BI)

### üîë Features
- Real-time ingestion of Bitcoin & Ethereum prices
- Streamed via Kafka into Snowflake tables
- Orchestrated using Airflow DAGs
- Metadata added (`dag_run_id`, `source`) for tracking
- Final reports built on Snowsight

### üìä Results
- End-to-end streaming pipeline successfully implemented
- Crypto price trends analyzed on Snowflake

üìÇ **Detailed Write-up** ‚Üí [view_project_folder](https://github.com/Jeevitak/Data_Engineer_Portfolio_V2/tree/main/real-time-crypto-pipeline)  

# Retail DataLakeHouse ETL Project üöÄ 
Overview: End-to-end retail data pipeline using Python, SQLite, dbt, and GitHub Actions CI/CD.

Role: Independent Developer
Tech: Python, SQLite, dbt, GitHub Actions
Project Overview:
Built an end-to-end retail data pipeline to generate, ingest, transform, and test sales data for analytics.

Impact / Highlights:
Automated ETL pipeline with CI/CD workflow on GitHub Actions
Implemented dbt models with data quality tests (nulls, uniqueness, calculations)
Enabled structured, clean data for downstream analytics and reporting

[View Project Folder](https://github.com/Jeevitak/Data_Engineer_Portfolio_V2/tree/main/Retail_DataLakeHouse)

## üì´ Contact Me
- **LinkedIn:** [linkedin.com/in/jeevita](https://www.linkedin.com/in/jeevita-kulkarni-326a62273/)  
- **Email:** jeevitakulkarni4@gmail.com
- **GitHub:** [github.com/Jeevitak](https://github.com/Jeevitak)  

---






